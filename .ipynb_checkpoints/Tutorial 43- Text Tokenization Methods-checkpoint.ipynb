{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "401bc008-f5ec-49fb-9ce9-1066ee2b96a4",
   "metadata": {},
   "source": [
    "### 1. Hugging Face Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81baf248-0142-40d9-a360-2815c2d833d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 2019, 2742, 6251, 1012,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = tokenizer(\"This is an example sentence.\", truncation = True, padding = True, return_tensors = 'pt')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e62fd-a644-4170-8e01-b4f53f858a32",
   "metadata": {},
   "source": [
    "### 2. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57d8e08-5787-462c-9c28-80ec44f6634c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This is an example sentence.\")\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737da6c-e19d-42b5-85e3-8a2b29aa5aa9",
   "metadata": {},
   "source": [
    "### 3. NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c0f73f-0fe6-43e1-9ab9-5084ccd1dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"This is an example sentence.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f5f42-7b4e-41c7-9483-8f112ddbea83",
   "metadata": {},
   "source": [
    "### 4. Stanford NLP (Stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50714bc5-2c4d-4cdc-882e-290db2f36995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b357d133af433c8d8d64b843eb1076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 09:35:37 INFO: Downloaded file to C:\\Users\\Ciya\\stanza_resources\\resources.json\n",
      "2025-01-01 09:35:37 INFO: Downloading default packages for language: en (English) ...\n",
      "2025-01-01 09:35:38 INFO: File exists: C:\\Users\\Ciya\\stanza_resources\\en\\default.zip\n",
      "2025-01-01 09:35:41 INFO: Finished downloading models and saved to C:\\Users\\Ciya\\stanza_resources\n",
      "2025-01-01 09:35:41 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2835491799c4bc48175aa797ee0a4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 09:35:41 INFO: Downloaded file to C:\\Users\\Ciya\\stanza_resources\\resources.json\n",
      "2025-01-01 09:35:41 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-01-01 09:35:41 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| mwt       | combined |\n",
      "========================\n",
      "\n",
      "2025-01-01 09:35:41 INFO: Using device: cpu\n",
      "2025-01-01 09:35:41 INFO: Loading: tokenize\n",
      "2025-01-01 09:35:41 INFO: Loading: mwt\n",
      "2025-01-01 09:35:41 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(lang = \"en\", processors = \"tokenize\")\n",
    "doc = nlp(\"This is an example sentence.\")\n",
    "tokens = [word.text for sentence in doc.sentences for word in sentence.words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fabeca-15b3-4d92-a8d7-d1855e409ebe",
   "metadata": {},
   "source": [
    "### 5. SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a02a95b2-3dd4-4043-a6aa-1fdf9ba55272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Th', 'is', '▁is', '▁an', '▁example', '▁set', 'ence', '.']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Train a SentencePiece model (if not using a pre-trained model)\n",
    "spm.SentencePieceTrainer.train(input = './data/data.txt', model_prefix = 'tokenizer', vocab_size = '868')\n",
    "\n",
    "# Load and tokenize\n",
    "sp = spm.SentencePieceProcessor(model_file = 'tokenizer.model')\n",
    "tokens = sp.encode(\"This is an example setence.\", out_type = str)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0c8fa-3c30-4037-961e-625a26b14f24",
   "metadata": {},
   "source": [
    "### 6. OpenAI Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "472eeafe-aaa9-49b3-9f5e-97327991a98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1212, 318, 281, 1672, 6827, 13], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokens = tokenizer(\"This is an example sentence.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b00a15-d7fe-40b3-97b4-125bd141ba32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
